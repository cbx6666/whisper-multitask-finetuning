data:
  # (id | language | task) -> wav_path, text
  train:
    wav_scp: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/train_wav.scp"
    text: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/train_text.txt"
  test:
    wav_scp: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/test_wav.scp"
    text: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/test_text.txt"

predict:
  model_path: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/model"
  eval:
    wav_scp: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/eval_wav.scp"
    text: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/eval_text.txt"
  result_file: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/result"

model:
  model_path: "/mnt/d/WSL/asr_large_model/whisper_model/whisper-large-v2"
  is_large_model: True
  data_collator:
    forward_attention_mask: False
  model_train_argv:
    out_model_path: "./model"
    resume_from_checkpoint: "./model"
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 1
    learning_rate: 0.0001
    warmup_steps: 50 
    num_train_epochs: 1
    evaluation_strategy: "epoch"
    fp16: False
    per_device_eval_batch_size: 1
    generation_max_length: 128 
    logging_steps: 2 
    remove_unused_columns: False
    label_names:
      - labels
