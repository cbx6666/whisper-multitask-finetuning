data:
  # (id | language | task) -> wav_path, text
  train:
    wav_scp: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/train_wav.scp"
    text: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/train_text.txt"
  test:
    wav_scp: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/test_wav.scp"
    text: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/test_text.txt"

predict:
  model_path: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/trained_model"
  eval:
    wav_scp: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/eval_wav.scp"
    text: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/eval_text.txt"
  result_file: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/result"

dev_env:
  # 原始模型
  ori_model_path: "/mnt/d/WSL/asr_large_model/whisper_model/whisper-large-v2"
  merged_model_path: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/merged_model"
  # 转后的模型
  ctranslate_model_path: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/ctranslate_model"
  conf:
    # ['cuda','cpu']
    device: "cuda"
    # ['float16','int8_float16(run on GPU with INT8)','int8']
    compute_type: "float32"
  result_file: "./data/result_fast"
  dev:
    wav_scp: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/eval_wav.scp"
    text: "/mnt/d/WSL/asr_large_model/whisper-multitask-finetuning/example/aishell-sample/data/eval_text.txt"

model:
  model_path: "/mnt/d/WSL/asr_large_model/whisper_model/whisper-large-v2"
  is_large_model: True
  data_collator:
    forward_attention_mask: False
  model_train_argv:
    out_model_path: "./model"
    resume_from_checkpoint: "./model"
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 1
    learning_rate: 0.000005
    warmup_steps: 50 
    num_train_epochs: 1
    evaluation_strategy: "epoch"
    fp16: False
    per_device_eval_batch_size: 1
    generation_max_length: 128 
    logging_steps: 100
    remove_unused_columns: False
    label_names:
      - labels